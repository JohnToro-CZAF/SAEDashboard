{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Download SAE with SAE Lens.\n",
    "2. Create a dataset consistent with that SAE. \n",
    "3. Fold the SAE decoder norm weights so that feature activations are \"correct\".\n",
    "4. Estimate the activation normalization constant if needed, and fold it into the SAE weights.\n",
    "5. Run the SAE generator for the features you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from sae_lens import SAE\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_dashboard.sae_vis_data import SaeVisConfig\n",
    "from sae_dashboard.sae_vis_runner import SaeVisRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Download / Initialize SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loading model config for meta-llama/Llama-3.2-1B-Instruct\n",
      "Loaded model config for {'d_model': 2048, 'd_head': 64, 'n_heads': 32, 'd_mlp': 8192, 'n_layers': 16, 'n_ctx': 2048, 'eps': 1e-05, 'd_vocab': 128256, 'act_fn': 'silu', 'n_key_value_heads': 8, 'normalization_type': 'RMS', 'positional_embedding_type': 'rotary', 'rotary_adjacent_pairs': False, 'rotary_dim': 64, 'final_rms': True, 'gated_mlp': True, 'original_architecture': 'LlamaForCausalLM', 'tokenizer_name': 'meta-llama/Llama-3.2-1B-Instruct'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.2-1B-Instruct into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836c1748a3dd42b99e9c32cccb980152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layers.8/cfg.json:   0%|          | 0.00/563 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Loading from eleuther-----\n",
      "/datadrive5/.cache/hub/models--huypn16--sae-llama-3.2-1B-32x/snapshots/c1d312b5f3e8b693867ef3089a83c6aae051ffc3/layers.8/cfg.json\n",
      "{'architecture': 'topk', 'hook_name': 'blocks.8.hook_resid_post', 'hook_layer': 8, 'layer': 8, 'k': 32, 'activation_fn_str': 'relu', 'd_sae': 65536, 'd_in': 2048, 'multi_topk': False, 'device': 'cuda', 'apply_b_dec_to_input': False, 'finetuning_scaling_factor': False, 'context_size': 1024, 'hook_head_index': None, 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'float32', 'sae_lens_training_version': 'eleuther', 'neuronpedia_id': None, 'activation_fn_kwargs': {}, 'model_from_pretrained_kwargs': {}}\n",
      "{'architecture': 'topk', 'hook_name': 'blocks.8.hook_resid_post', 'hook_layer': 8, 'layer': 8, 'k': 32, 'activation_fn_str': 'relu', 'd_sae': 65536, 'd_in': 2048, 'multi_topk': False, 'device': 'cuda', 'apply_b_dec_to_input': False, 'finetuning_scaling_factor': False, 'context_size': 1024, 'hook_head_index': None, 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'float32', 'sae_lens_training_version': 'eleuther', 'neuronpedia_id': None, 'activation_fn_kwargs': {}, 'model_from_pretrained_kwargs': {}, 'model_name': '', 'dataset_path': '', 'dataset_trust_remote_code': False}\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "model = HookedTransformer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", device=device, dtype=\"bfloat16\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "sae, cfg_dict, sparsity = SAE.from_eleuther(\n",
    "    release=\"huypn16/sae-llama-3.2-1B-32x\",  # see other options in sae_lens/pretrained_saes.yaml\n",
    "    sae_id=\"layers.8\",  # won't always be a hook point\n",
    "    device=device,\n",
    ")\n",
    "# fold w_dec norm so feature activations are accurate\n",
    "sae.fold_W_dec_norm()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get token dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token columns: ['problem', 'solution']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datadrive5/huypn16/anaconda3/envs/ana/lib/python3.11/site-packages/sae_lens/training/activations_store.py:265: UserWarning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://jbloomaus.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sae_lens import ActivationsStore\n",
    "# activations_store = ActivationsStore.from_sae(\n",
    "#     model=model,\n",
    "#     sae=sae,\n",
    "#     streaming=True,\n",
    "#     dataset=\"open-web-math/open-web-math\",\n",
    "#     token_columns=[\"text\"],\n",
    "#     store_batch_size_prompts=16,\n",
    "#     n_batches_in_buffer=16,\n",
    "#     device=device,\n",
    "# )\n",
    "activations_store = ActivationsStore.from_sae(\n",
    "    model=model,\n",
    "    sae=sae,\n",
    "    streaming=True,\n",
    "    dataset=\"lighteval/MATH\",\n",
    "    token_columns=[\"problem\", \"solution\"],\n",
    "    store_batch_size_prompts=16,\n",
    "    n_batches_in_buffer=16,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>),SE);\n",
      "label(\"G\",(4,3),SW);\n",
      "label(\"6\",(3,0),S);\n",
      "label(\"1\",(0.5,3),N);\n",
      "label(\"2\",(5,3),N);\n",
      "label(\"3\",(6,1.5),E);\n",
      "[/asy]\n",
      "solution: We first find the length of line segment $FG$. Since $DC$ has length $6$ and $DF$ and $GC$ have lengths $1$ and $2$ respectively, $FG$ must have length $3$. Next, we notice that $DC$ and $AB$ are parallel so $\\angle EFG \\cong \\angle EAB$ because they are corresponding angles. Similarly, $\\angle EGF \\cong \\angle EBA$. Now that we have two pairs of congruent angles, we know that $\\triangle FEG \\sim \\triangle AEB$ by Angle-Angle Similarity.\n",
      "\n",
      "Because the two triangles are similar, we have that the ratio of the altitudes of $\\triangle FEG$ to $\\triangle AEB$ equals the ratio of the bases. $FG:AB=3:6=1:2$, so the the ratio of the altitude of $\\triangle FEG$ to that of $\\triangle AEB$ is also $1:2$. Thus, the height of the rectangle $ABCD$ must be half of the altitude of $\\triangle AEB$. Since the height of rectangle $ABCD$ is $3$, the altitude of $\\triangle AEB$ must be $6$. Now that we know that the base and altitude of $\\triangle AEB$ are both $6$, we know that the area of triangle $AEB$ is equal to $\\frac{1}{2}$base $\\times$ height $= (\\frac{1}{2})(6)(6) = \\boxed{18}$ square units.<|begin_of_text|>problem: Let triangle $ABC$ be a right triangle in the xy-plane with a right angle at $C$. Given that the length of the hypotenuse $AB$ is $60$, and that the medians through $A$ and $B$ lie along the lines $y=x+3$ and $y=2x+4$ respectively, find the area of triangle $ABC$.\n",
      "\n",
      "solution: Translate so the medians are $y = x$, and $y = 2x$, then model the points $A: (a,a)$ and $B: (b,2b)$. $(0,0)$ is the centroid, and is the average of the vertices, so $C: (- a - b, - a - 2b)$\n",
      "$AB = 60$ so\n",
      "$3600 = (a - b)^2 + (2b - a)^2$\n",
      "$3600 = 2a^2 + 5b^2 - 6ab \\ \\ \\ \\ (1)$\n",
      "$AC$ and $BC$ are perpendicular, so the product of their slopes is $-1$, giving\n",
      "$\\left(\\frac {2a + 2b}{2a + b}\\right)\\left(\\frac {a + 4b}{a + 2b}\\right) = - 1$\n",
      "$2a^2 + 5b^2 = - \\frac {15}{2}ab \\ \\ \\ \\  (2)$\n",
      "Combining $(1)$ and $(2)$, we get $ab = - \\frac {800}{3}$\n",
      "Using the determinant product for area of a triangle (this simplifies nicely, add columns 1 and 2, add rows 2 and 3), the area is $\\left|\\frac {3}{2}ab\\right|$, so we get the answer to be $\\boxed{400}$.<|begin_of_text|>problem: In $\\triangle ABC$, $AB= 425$, $BC=450$, and $AC=510$. An interior point $P$ is then drawn, and segments are drawn through $P$ parallel to the sides of the triangle. If these three segments are of an equal length $d$, find $d$.\n",
      "\n",
      "solution: [asy] size(200); pathpen = black; pointpen = black +linewidth(0.6); pen s = fontsize(10); pair C=(0,0),A=(510,0),B=IP(circle(C,450),circle(A,425)); /* construct remaining points */ pair Da=IP(Circle(A,289),A--B),E=IP(Circle(C,324),B--C),Ea=IP(Circle(B,270),B--C); pair D=IP(Ea--(Ea+A-C),A--B),F=IP(Da--(Da+C-B),A--C),Fa=IP(E--(E+A-B),A--C);  D(MP(\"A\",A,s)--MP(\"B\",B,N,s)--MP(\"C\",C,s)--cycle); dot(M\n",
      "torch.Size([16384, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2332336/1266841329.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  token_dataset = torch.load(\"llama_lighteval.pt\")\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "def get_tokens(\n",
    "    activations_store: ActivationsStore,\n",
    "    n_prompts: int,\n",
    "):\n",
    "    all_tokens_list = []\n",
    "    pbar = tqdm(range(n_prompts))\n",
    "    for _ in pbar:\n",
    "        batch_tokens = activations_store.get_batch_tokens()\n",
    "        batch_tokens = batch_tokens[torch.randperm(batch_tokens.shape[0])][\n",
    "            : batch_tokens.shape[0]\n",
    "        ]\n",
    "        all_tokens_list.append(batch_tokens)\n",
    "\n",
    "    all_tokens = torch.cat(all_tokens_list, dim=0)\n",
    "    all_tokens = all_tokens[torch.randperm(all_tokens.shape[0])]\n",
    "    return all_tokens\n",
    "\n",
    "# 1000 prompts is plenty for a demo.\n",
    "if os.path.exists(\"llama_lighteval.pt\"):\n",
    "    token_dataset = torch.load(\"llama_lighteval.pt\")\n",
    "else:\n",
    "    token_dataset = get_tokens(activations_store, n_prompts=1024)\n",
    "    torch.save(token_dataset, \"llama_lighteval.pt\")\n",
    "\n",
    "print(tokenizer.decode(token_dataset[0]))\n",
    "print(token_dataset.shape) # [store_batch_size_prompts * n_prompts, 1024]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 Evaluate the SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard replacement hook:  blocks.8.hook_resid_post\n",
      "standard replacement hook:  blocks.8.hook_resid_post\n",
      "standard replacement hook:  blocks.8.hook_resid_post\n",
      "standard replacement hook:  blocks.8.hook_resid_post\n",
      "standard replacement hook:  blocks.8.hook_resid_post\n",
      "standard replacement hook:  blocks.8.hook_resid_post\n",
      "standard replacement hook:  blocks.8.hook_resid_post\n",
      "standard replacement hook:  blocks.8.hook_resid_post\n",
      "standard replacement hook:  blocks.8.hook_resid_post\n",
      "standard replacement hook:  blocks.8.hook_resid_post\n",
      "{\n",
      "    \"metrics/kl_div_with_sae\": 0.78125,\n",
      "    \"metrics/kl_div_with_ablation\": 10.4375,\n",
      "    \"metrics/ce_loss_with_sae\": 2.25,\n",
      "    \"metrics/ce_loss_without_sae\": 1.5703125,\n",
      "    \"metrics/ce_loss_with_ablation\": 11.75,\n",
      "    \"metrics/kl_div_score\": 0.9251497005988024,\n",
      "    \"metrics/ce_loss_score\": 0.9332310053722179,\n",
      "    \"metrics/l2_norm_in\": 9.25,\n",
      "    \"metrics/l2_norm_out\": 8.761981964111328,\n",
      "    \"metrics/l2_ratio\": 0.9328801035881042,\n",
      "    \"metrics/l0\": 32.0,\n",
      "    \"metrics/l1\": 34.63883972167969,\n",
      "    \"metrics/explained_variance\": 0.632268488407135,\n",
      "    \"metrics/mse\": 0.004027680493891239,\n",
      "    \"metrics/total_tokens_evaluated\": 81920\n",
      "}\n",
      "0.9332310053722179\n",
      "1.5703125\n",
      "2.25\n"
     ]
    }
   ],
   "source": [
    "from sae_lens import run_evals\n",
    "from sae_lens.evals import get_eval_everything_config\n",
    "\n",
    "eval_metrics = run_evals(\n",
    "    sae=sae,\n",
    "    activation_store=activations_store,\n",
    "    model=model,\n",
    "    eval_config=get_eval_everything_config(\n",
    "        batch_size_prompts=8,\n",
    "        n_eval_reconstruction_batches=10,\n",
    "        n_eval_sparsity_variance_batches=3,\n",
    "    )\n",
    ")\n",
    "print(json.dumps(eval_metrics, indent=4))\n",
    "# CE Loss score should be high for residual stream SAEs\n",
    "print(eval_metrics[\"metrics/ce_loss_score\"])\n",
    "# ce loss without SAE should be fairly low < 3.5 suggesting the Model is being run correctly\n",
    "print(eval_metrics[\"metrics/ce_loss_without_sae\"])\n",
    "# ce loss with SAE shouldn't be massively higher\n",
    "print(eval_metrics[\"metrics/ce_loss_with_sae\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Generate Feature Dashboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_token_batches: 256\n",
      "len(feature_batches): 1\n",
      "len(tokens): 4096\n",
      "cfg.minibatch_size_tokens: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12384fc70674342a5269492f8091b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Forward passes to cache data for vis:   0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9c6f1e139c4ff7912241ccec483f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting vis data from cached data:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_model_acts_time 0.08502626419067383\n",
      "encode_time 0.004608154296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datadrive5/huypn16-backup/ReinforceLLM/SAEDashboard/sae_dashboard/feature_data_generator.py:224: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(\n",
      "/datadrive5/huypn16/anaconda3/envs/ana/lib/python3.11/site-packages/sae_lens/sae.py:333: UserWarning: Less than k features in the input. Testing feature-centric only.\n",
      "  warnings.warn(\"Less than k features in the input. Testing feature-centric only.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postprocessing_time 0.337327241897583\n",
      "get_model_acts_time 0.0870199203491211\n",
      "encode_time 0.0022530555725097656\n",
      "postprocessing_time 0.33298230171203613\n",
      "get_model_acts_time 0.0524289608001709\n",
      "encode_time 0.002359151840209961\n",
      "postprocessing_time 0.31594014167785645\n",
      "get_model_acts_time 0.053992509841918945\n",
      "encode_time 0.0021483898162841797\n",
      "postprocessing_time 0.3226654529571533\n",
      "get_model_acts_time 0.05787801742553711\n",
      "encode_time 0.010416746139526367\n",
      "postprocessing_time 0.3277270793914795\n",
      "get_model_acts_time 0.05559968948364258\n",
      "encode_time 0.0020656585693359375\n",
      "postprocessing_time 0.32517290115356445\n",
      "get_model_acts_time 0.06272339820861816\n",
      "encode_time 0.002248525619506836\n",
      "postprocessing_time 0.32788968086242676\n",
      "get_model_acts_time 0.05564689636230469\n",
      "encode_time 0.002056598663330078\n",
      "postprocessing_time 0.33024096488952637\n",
      "get_model_acts_time 0.0579228401184082\n",
      "encode_time 0.002100706100463867\n",
      "postprocessing_time 0.31865477561950684\n",
      "get_model_acts_time 0.05360674858093262\n",
      "encode_time 0.0020623207092285156\n",
      "postprocessing_time 0.32774925231933594\n",
      "get_model_acts_time 0.05411386489868164\n",
      "encode_time 0.002917051315307617\n",
      "postprocessing_time 0.3178689479827881\n",
      "get_model_acts_time 0.05458831787109375\n",
      "encode_time 0.002064228057861328\n",
      "postprocessing_time 0.32167792320251465\n",
      "get_model_acts_time 0.055577993392944336\n",
      "encode_time 0.0020475387573242188\n",
      "postprocessing_time 0.31270766258239746\n",
      "get_model_acts_time 0.05479764938354492\n",
      "encode_time 0.0020835399627685547\n",
      "postprocessing_time 0.32819175720214844\n",
      "get_model_acts_time 0.05495500564575195\n",
      "encode_time 0.0022161006927490234\n",
      "postprocessing_time 0.32758355140686035\n",
      "get_model_acts_time 0.05513501167297363\n",
      "encode_time 0.0021376609802246094\n",
      "postprocessing_time 0.36083245277404785\n",
      "get_model_acts_time 0.055321455001831055\n",
      "encode_time 0.002866983413696289\n",
      "postprocessing_time 0.3213822841644287\n",
      "get_model_acts_time 0.05706954002380371\n",
      "encode_time 0.0020775794982910156\n",
      "postprocessing_time 0.32511138916015625\n",
      "get_model_acts_time 0.05447030067443848\n",
      "encode_time 0.0020475387573242188\n",
      "postprocessing_time 0.3321826457977295\n",
      "get_model_acts_time 0.05382084846496582\n",
      "encode_time 0.0020635128021240234\n",
      "postprocessing_time 0.3210153579711914\n",
      "get_model_acts_time 0.053801536560058594\n",
      "encode_time 0.0020530223846435547\n",
      "postprocessing_time 0.33358263969421387\n",
      "get_model_acts_time 0.055262088775634766\n",
      "encode_time 0.0022745132446289062\n",
      "postprocessing_time 0.3215174674987793\n",
      "get_model_acts_time 0.05255413055419922\n",
      "encode_time 0.002416849136352539\n",
      "postprocessing_time 0.33146047592163086\n",
      "get_model_acts_time 0.053963661193847656\n",
      "encode_time 0.0020461082458496094\n",
      "postprocessing_time 0.3304321765899658\n",
      "get_model_acts_time 0.061966896057128906\n",
      "encode_time 0.0022749900817871094\n",
      "postprocessing_time 0.3406953811645508\n",
      "get_model_acts_time 0.05619168281555176\n",
      "encode_time 0.0021088123321533203\n",
      "postprocessing_time 0.31345105171203613\n",
      "get_model_acts_time 0.05397748947143555\n",
      "encode_time 0.002087831497192383\n",
      "postprocessing_time 0.32485151290893555\n",
      "get_model_acts_time 0.05297112464904785\n",
      "encode_time 0.0020644664764404297\n",
      "postprocessing_time 0.32294440269470215\n",
      "get_model_acts_time 0.05288195610046387\n",
      "encode_time 0.002524852752685547\n",
      "postprocessing_time 0.31581997871398926\n",
      "get_model_acts_time 0.05317425727844238\n",
      "encode_time 0.0020771026611328125\n",
      "postprocessing_time 0.3350348472595215\n",
      "get_model_acts_time 0.05236053466796875\n",
      "encode_time 0.0020630359649658203\n",
      "postprocessing_time 0.32340335845947266\n",
      "get_model_acts_time 0.0502324104309082\n",
      "encode_time 0.002111196517944336\n",
      "postprocessing_time 0.3354637622833252\n",
      "get_model_acts_time 0.05745410919189453\n",
      "encode_time 0.0020737648010253906\n",
      "postprocessing_time 0.3629906177520752\n",
      "get_model_acts_time 0.05285143852233887\n",
      "encode_time 0.0020797252655029297\n",
      "postprocessing_time 0.329348087310791\n",
      "get_model_acts_time 0.053575754165649414\n",
      "encode_time 0.002428293228149414\n",
      "postprocessing_time 0.3055551052093506\n",
      "get_model_acts_time 0.06002974510192871\n",
      "encode_time 0.002299785614013672\n",
      "postprocessing_time 0.3240396976470947\n",
      "get_model_acts_time 0.05472564697265625\n",
      "encode_time 0.0021092891693115234\n",
      "postprocessing_time 0.3247406482696533\n",
      "get_model_acts_time 0.0604398250579834\n",
      "encode_time 0.002214670181274414\n",
      "postprocessing_time 0.32503557205200195\n",
      "get_model_acts_time 0.0604856014251709\n",
      "encode_time 0.0021314620971679688\n",
      "postprocessing_time 0.330899715423584\n",
      "get_model_acts_time 0.05661344528198242\n",
      "encode_time 0.002110719680786133\n",
      "postprocessing_time 0.3544342517852783\n",
      "get_model_acts_time 0.055349111557006836\n",
      "encode_time 0.0025293827056884766\n",
      "postprocessing_time 0.32883787155151367\n",
      "get_model_acts_time 0.05679821968078613\n",
      "encode_time 0.0020875930786132812\n",
      "postprocessing_time 0.3177673816680908\n",
      "get_model_acts_time 0.05240011215209961\n",
      "encode_time 0.0022928714752197266\n",
      "postprocessing_time 0.32743096351623535\n",
      "get_model_acts_time 0.05170488357543945\n",
      "encode_time 0.0022096633911132812\n",
      "postprocessing_time 0.32254767417907715\n",
      "get_model_acts_time 0.05276131629943848\n",
      "encode_time 0.0022950172424316406\n",
      "postprocessing_time 0.3391759395599365\n",
      "get_model_acts_time 0.0516054630279541\n",
      "encode_time 0.0020966529846191406\n",
      "postprocessing_time 0.32151079177856445\n",
      "get_model_acts_time 0.053114891052246094\n",
      "encode_time 0.002583742141723633\n",
      "postprocessing_time 0.33079957962036133\n",
      "get_model_acts_time 0.049902915954589844\n",
      "encode_time 0.0020554065704345703\n",
      "postprocessing_time 0.323610782623291\n",
      "get_model_acts_time 0.04959726333618164\n",
      "encode_time 0.0021162033081054688\n",
      "postprocessing_time 0.3240337371826172\n",
      "get_model_acts_time 0.054696083068847656\n",
      "encode_time 0.0020558834075927734\n",
      "postprocessing_time 0.35741734504699707\n",
      "get_model_acts_time 0.04862570762634277\n",
      "encode_time 0.0020339488983154297\n",
      "postprocessing_time 0.32449793815612793\n",
      "get_model_acts_time 0.05301046371459961\n",
      "encode_time 0.002294778823852539\n",
      "postprocessing_time 0.3319132328033447\n",
      "get_model_acts_time 0.05080771446228027\n",
      "encode_time 0.002429485321044922\n",
      "postprocessing_time 0.3304746150970459\n",
      "get_model_acts_time 0.024213314056396484\n",
      "encode_time 0.0023679733276367188\n",
      "postprocessing_time 0.3497500419616699\n",
      "get_model_acts_time 0.02248859405517578\n",
      "encode_time 0.0021314620971679688\n",
      "postprocessing_time 0.3438711166381836\n",
      "get_model_acts_time 0.020832061767578125\n",
      "encode_time 0.0021462440490722656\n",
      "postprocessing_time 0.3328409194946289\n",
      "get_model_acts_time 0.021600008010864258\n",
      "encode_time 0.0021979808807373047\n",
      "postprocessing_time 0.3606092929840088\n",
      "get_model_acts_time 0.020579099655151367\n",
      "encode_time 0.002118825912475586\n",
      "postprocessing_time 0.35312366485595703\n",
      "get_model_acts_time 0.023764610290527344\n",
      "encode_time 0.002691507339477539\n",
      "postprocessing_time 0.3425934314727783\n",
      "get_model_acts_time 0.021358251571655273\n",
      "encode_time 0.002084493637084961\n",
      "postprocessing_time 0.3782825469970703\n",
      "get_model_acts_time 0.034929513931274414\n",
      "encode_time 0.0021181106567382812\n",
      "postprocessing_time 0.3485143184661865\n",
      "get_model_acts_time 0.035761356353759766\n",
      "encode_time 0.0020949840545654297\n",
      "postprocessing_time 0.3407149314880371\n",
      "get_model_acts_time 0.02197122573852539\n",
      "encode_time 0.0024569034576416016\n",
      "postprocessing_time 0.3672022819519043\n",
      "get_model_acts_time 0.04994463920593262\n",
      "encode_time 0.002131938934326172\n",
      "postprocessing_time 0.32839179039001465\n",
      "get_model_acts_time 0.051320552825927734\n",
      "encode_time 0.002478361129760742\n",
      "postprocessing_time 0.3321974277496338\n",
      "get_model_acts_time 0.0482790470123291\n",
      "encode_time 0.002093791961669922\n",
      "postprocessing_time 0.33924269676208496\n",
      "get_model_acts_time 0.05034327507019043\n",
      "encode_time 0.002084016799926758\n",
      "postprocessing_time 0.32587218284606934\n",
      "get_model_acts_time 0.04907560348510742\n",
      "encode_time 0.0021898746490478516\n",
      "postprocessing_time 0.3248286247253418\n",
      "get_model_acts_time 0.05033135414123535\n",
      "encode_time 0.0020940303802490234\n",
      "postprocessing_time 0.3319089412689209\n",
      "get_model_acts_time 0.04747605323791504\n",
      "encode_time 0.0020618438720703125\n",
      "postprocessing_time 0.3262603282928467\n",
      "get_model_acts_time 0.05147123336791992\n",
      "encode_time 0.0024652481079101562\n",
      "postprocessing_time 0.3228342533111572\n",
      "get_model_acts_time 0.05150341987609863\n",
      "encode_time 0.002100706100463867\n",
      "postprocessing_time 0.35642051696777344\n",
      "get_model_acts_time 0.05735611915588379\n",
      "encode_time 0.0021402835845947266\n",
      "postprocessing_time 0.3753492832183838\n",
      "get_model_acts_time 0.05530834197998047\n",
      "encode_time 0.0020639896392822266\n",
      "postprocessing_time 0.321002721786499\n",
      "get_model_acts_time 0.053635597229003906\n",
      "encode_time 0.002300262451171875\n",
      "postprocessing_time 0.3294386863708496\n",
      "get_model_acts_time 0.054680585861206055\n",
      "encode_time 0.0020742416381835938\n",
      "postprocessing_time 0.3196544647216797\n",
      "get_model_acts_time 0.04954957962036133\n",
      "encode_time 0.0024576187133789062\n",
      "postprocessing_time 0.3297090530395508\n",
      "get_model_acts_time 0.05706644058227539\n",
      "encode_time 0.0020799636840820312\n",
      "postprocessing_time 0.32947421073913574\n",
      "get_model_acts_time 0.04683852195739746\n",
      "encode_time 0.0020689964294433594\n",
      "postprocessing_time 0.32285094261169434\n",
      "get_model_acts_time 0.044431447982788086\n",
      "encode_time 0.0022487640380859375\n",
      "postprocessing_time 0.32859277725219727\n",
      "get_model_acts_time 0.059316396713256836\n",
      "encode_time 0.002079486846923828\n",
      "postprocessing_time 0.32189226150512695\n",
      "get_model_acts_time 0.05339670181274414\n",
      "encode_time 0.002101421356201172\n",
      "postprocessing_time 0.3296480178833008\n",
      "get_model_acts_time 0.04377627372741699\n",
      "encode_time 0.00251007080078125\n",
      "postprocessing_time 0.33504343032836914\n",
      "get_model_acts_time 0.04507255554199219\n",
      "encode_time 0.0020694732666015625\n",
      "postprocessing_time 0.32743096351623535\n",
      "get_model_acts_time 0.04539752006530762\n",
      "encode_time 0.002077341079711914\n",
      "postprocessing_time 0.32342529296875\n",
      "get_model_acts_time 0.05062055587768555\n",
      "encode_time 0.002053499221801758\n",
      "postprocessing_time 0.3287789821624756\n",
      "get_model_acts_time 0.05048561096191406\n",
      "encode_time 0.002105236053466797\n",
      "postprocessing_time 0.3306925296783447\n",
      "get_model_acts_time 0.046946048736572266\n",
      "encode_time 0.0021276473999023438\n",
      "postprocessing_time 0.3199348449707031\n",
      "get_model_acts_time 0.0461881160736084\n",
      "encode_time 0.004944801330566406\n",
      "postprocessing_time 0.32518911361694336\n",
      "get_model_acts_time 0.052782535552978516\n",
      "encode_time 0.002044677734375\n",
      "postprocessing_time 0.3256227970123291\n",
      "get_model_acts_time 0.055440664291381836\n",
      "encode_time 0.002044677734375\n",
      "postprocessing_time 0.34473395347595215\n",
      "get_model_acts_time 0.061548709869384766\n",
      "encode_time 0.0020503997802734375\n",
      "postprocessing_time 0.33610987663269043\n",
      "get_model_acts_time 0.05451774597167969\n",
      "encode_time 0.0020804405212402344\n",
      "postprocessing_time 0.33307933807373047\n",
      "get_model_acts_time 0.05135488510131836\n",
      "encode_time 0.002053499221801758\n",
      "postprocessing_time 0.32413434982299805\n",
      "get_model_acts_time 0.059583425521850586\n",
      "encode_time 0.0024437904357910156\n",
      "postprocessing_time 0.32509613037109375\n",
      "get_model_acts_time 0.052988529205322266\n",
      "encode_time 0.0020279884338378906\n",
      "postprocessing_time 0.3334836959838867\n",
      "get_model_acts_time 0.061716318130493164\n",
      "encode_time 0.0020389556884765625\n",
      "postprocessing_time 0.3305068016052246\n",
      "get_model_acts_time 0.05529022216796875\n",
      "encode_time 0.0020923614501953125\n",
      "postprocessing_time 0.33054113388061523\n",
      "get_model_acts_time 0.058086395263671875\n",
      "encode_time 0.0022280216217041016\n",
      "postprocessing_time 0.3326888084411621\n",
      "get_model_acts_time 0.051076412200927734\n",
      "encode_time 0.002368927001953125\n",
      "postprocessing_time 0.32904052734375\n",
      "get_model_acts_time 0.051340341567993164\n",
      "encode_time 0.0025458335876464844\n",
      "postprocessing_time 0.32930564880371094\n",
      "get_model_acts_time 0.04824256896972656\n",
      "encode_time 0.0021004676818847656\n",
      "postprocessing_time 0.32740211486816406\n",
      "get_model_acts_time 0.04997444152832031\n",
      "encode_time 0.0020799636840820312\n",
      "postprocessing_time 0.33417725563049316\n",
      "get_model_acts_time 0.05214667320251465\n",
      "encode_time 0.0020554065704345703\n",
      "postprocessing_time 0.3235478401184082\n",
      "get_model_acts_time 0.051856279373168945\n",
      "encode_time 0.002064228057861328\n",
      "postprocessing_time 0.32621288299560547\n",
      "get_model_acts_time 0.05117368698120117\n",
      "encode_time 0.002101421356201172\n",
      "postprocessing_time 0.33318257331848145\n",
      "get_model_acts_time 0.05710792541503906\n",
      "encode_time 0.002434253692626953\n",
      "postprocessing_time 0.3235042095184326\n",
      "get_model_acts_time 0.04902005195617676\n",
      "encode_time 0.002099275588989258\n",
      "postprocessing_time 0.3234443664550781\n",
      "get_model_acts_time 0.05327939987182617\n",
      "encode_time 0.002103567123413086\n",
      "postprocessing_time 0.32442498207092285\n",
      "get_model_acts_time 0.05313539505004883\n",
      "encode_time 0.002105236053466797\n",
      "postprocessing_time 0.32433342933654785\n",
      "get_model_acts_time 0.05724692344665527\n",
      "encode_time 0.0020973682403564453\n",
      "postprocessing_time 0.32993388175964355\n",
      "get_model_acts_time 0.05767202377319336\n",
      "encode_time 0.0020401477813720703\n",
      "postprocessing_time 0.3410170078277588\n",
      "get_model_acts_time 0.030733346939086914\n",
      "encode_time 0.0027647018432617188\n",
      "postprocessing_time 0.336500883102417\n",
      "get_model_acts_time 0.05650663375854492\n",
      "encode_time 0.002048969268798828\n",
      "postprocessing_time 0.32084035873413086\n",
      "get_model_acts_time 0.052550554275512695\n",
      "encode_time 0.0020554065704345703\n",
      "postprocessing_time 0.33744168281555176\n",
      "get_model_acts_time 0.051450252532958984\n",
      "encode_time 0.0020759105682373047\n",
      "postprocessing_time 0.3326573371887207\n",
      "get_model_acts_time 0.05357813835144043\n",
      "encode_time 0.0020911693572998047\n",
      "postprocessing_time 0.32302379608154297\n",
      "get_model_acts_time 0.05312061309814453\n",
      "encode_time 0.0020780563354492188\n",
      "postprocessing_time 0.32712244987487793\n",
      "get_model_acts_time 0.04965376853942871\n",
      "encode_time 0.05213356018066406\n",
      "postprocessing_time 0.33730363845825195\n",
      "get_model_acts_time 0.058267831802368164\n",
      "encode_time 0.002079010009765625\n",
      "postprocessing_time 0.3239781856536865\n",
      "get_model_acts_time 0.04896855354309082\n",
      "encode_time 0.0020706653594970703\n",
      "postprocessing_time 0.32611656188964844\n",
      "get_model_acts_time 0.0405423641204834\n",
      "encode_time 0.002071380615234375\n",
      "postprocessing_time 0.32597804069519043\n",
      "get_model_acts_time 0.04758644104003906\n",
      "encode_time 0.0021181106567382812\n",
      "postprocessing_time 0.33776164054870605\n",
      "get_model_acts_time 0.048304080963134766\n",
      "encode_time 0.002080202102661133\n",
      "postprocessing_time 0.3237345218658447\n",
      "get_model_acts_time 0.0538790225982666\n",
      "encode_time 0.0024602413177490234\n",
      "postprocessing_time 0.40035462379455566\n",
      "get_model_acts_time 0.06147408485412598\n",
      "encode_time 0.0020737648010253906\n",
      "postprocessing_time 0.3306887149810791\n",
      "get_model_acts_time 0.04911351203918457\n",
      "encode_time 0.0020678043365478516\n",
      "postprocessing_time 0.3256969451904297\n",
      "get_model_acts_time 0.053072214126586914\n",
      "encode_time 0.002079010009765625\n",
      "postprocessing_time 0.3317728042602539\n",
      "get_model_acts_time 0.05942416191101074\n",
      "encode_time 0.0020744800567626953\n",
      "postprocessing_time 0.33308959007263184\n",
      "get_model_acts_time 0.05157637596130371\n",
      "encode_time 0.002048969268798828\n",
      "postprocessing_time 0.3224000930786133\n",
      "get_model_acts_time 0.04964709281921387\n",
      "encode_time 0.0024461746215820312\n",
      "postprocessing_time 0.322643518447876\n",
      "get_model_acts_time 0.05533957481384277\n",
      "encode_time 0.002054929733276367\n",
      "postprocessing_time 0.3260204792022705\n",
      "get_model_acts_time 0.059253692626953125\n",
      "encode_time 0.002050638198852539\n",
      "postprocessing_time 0.3247382640838623\n",
      "get_model_acts_time 0.05833292007446289\n",
      "encode_time 0.0022678375244140625\n",
      "postprocessing_time 0.32599401473999023\n",
      "get_model_acts_time 0.0470888614654541\n",
      "encode_time 0.0020873546600341797\n",
      "postprocessing_time 0.33471202850341797\n",
      "get_model_acts_time 0.04629039764404297\n",
      "encode_time 0.002084016799926758\n",
      "postprocessing_time 0.33077335357666016\n",
      "get_model_acts_time 0.05408143997192383\n",
      "encode_time 0.002635955810546875\n",
      "postprocessing_time 0.33273935317993164\n",
      "get_model_acts_time 0.0539705753326416\n",
      "encode_time 0.0020728111267089844\n",
      "postprocessing_time 0.3385195732116699\n",
      "get_model_acts_time 0.04904532432556152\n",
      "encode_time 0.0020627975463867188\n",
      "postprocessing_time 0.3387303352355957\n",
      "get_model_acts_time 0.05189919471740723\n",
      "encode_time 0.0020792484283447266\n",
      "postprocessing_time 0.3253164291381836\n",
      "get_model_acts_time 0.04658365249633789\n",
      "encode_time 0.0020945072174072266\n",
      "postprocessing_time 0.32337307929992676\n",
      "get_model_acts_time 0.04886889457702637\n",
      "encode_time 0.002329111099243164\n",
      "postprocessing_time 0.32423853874206543\n",
      "get_model_acts_time 0.04677534103393555\n",
      "encode_time 0.0025322437286376953\n",
      "postprocessing_time 0.3245251178741455\n",
      "get_model_acts_time 0.05727076530456543\n",
      "encode_time 0.0020787715911865234\n",
      "postprocessing_time 0.3372941017150879\n",
      "get_model_acts_time 0.051409006118774414\n",
      "encode_time 0.002042055130004883\n",
      "postprocessing_time 0.33130359649658203\n",
      "get_model_acts_time 0.047742605209350586\n",
      "encode_time 0.0020525455474853516\n",
      "postprocessing_time 0.3266031742095947\n",
      "get_model_acts_time 0.04964399337768555\n",
      "encode_time 0.0020880699157714844\n",
      "postprocessing_time 0.33457255363464355\n",
      "get_model_acts_time 0.049922943115234375\n",
      "encode_time 0.002109527587890625\n",
      "postprocessing_time 0.3211805820465088\n",
      "get_model_acts_time 0.048552751541137695\n",
      "encode_time 0.002471446990966797\n",
      "postprocessing_time 0.3336787223815918\n",
      "get_model_acts_time 0.04486870765686035\n",
      "encode_time 0.002059459686279297\n",
      "postprocessing_time 0.32956814765930176\n",
      "get_model_acts_time 0.04650092124938965\n",
      "encode_time 0.0020647048950195312\n",
      "postprocessing_time 0.32251977920532227\n",
      "get_model_acts_time 0.04741311073303223\n",
      "encode_time 0.002068758010864258\n",
      "postprocessing_time 0.3230748176574707\n",
      "get_model_acts_time 0.05090165138244629\n",
      "encode_time 0.002051830291748047\n",
      "postprocessing_time 0.3274357318878174\n",
      "get_model_acts_time 0.05078005790710449\n",
      "encode_time 0.002118825912475586\n",
      "postprocessing_time 0.33773040771484375\n",
      "get_model_acts_time 0.05274343490600586\n",
      "encode_time 0.0024690628051757812\n",
      "postprocessing_time 0.33599352836608887\n",
      "get_model_acts_time 0.053615570068359375\n",
      "encode_time 0.0021910667419433594\n",
      "postprocessing_time 0.3384530544281006\n",
      "get_model_acts_time 0.05730772018432617\n",
      "encode_time 0.0020744800567626953\n",
      "postprocessing_time 0.3553626537322998\n",
      "get_model_acts_time 0.041413307189941406\n",
      "encode_time 0.002068758010864258\n",
      "postprocessing_time 0.33875060081481934\n",
      "get_model_acts_time 0.052728891372680664\n",
      "encode_time 0.0021104812622070312\n",
      "postprocessing_time 0.3299126625061035\n",
      "get_model_acts_time 0.05141758918762207\n",
      "encode_time 0.0020399093627929688\n",
      "postprocessing_time 0.32174205780029297\n",
      "get_model_acts_time 0.04481649398803711\n",
      "encode_time 0.0024390220642089844\n",
      "postprocessing_time 0.3204383850097656\n",
      "get_model_acts_time 0.04138541221618652\n",
      "encode_time 0.0020973682403564453\n",
      "postprocessing_time 0.32506799697875977\n",
      "get_model_acts_time 0.04603457450866699\n",
      "encode_time 0.0020897388458251953\n",
      "postprocessing_time 0.33385682106018066\n",
      "get_model_acts_time 0.04532265663146973\n",
      "encode_time 0.0020837783813476562\n",
      "postprocessing_time 0.3419947624206543\n",
      "get_model_acts_time 0.04866790771484375\n",
      "encode_time 0.0020515918731689453\n",
      "postprocessing_time 0.3206136226654053\n",
      "get_model_acts_time 0.04950857162475586\n",
      "encode_time 0.002043485641479492\n",
      "postprocessing_time 0.3258802890777588\n",
      "get_model_acts_time 0.04435467720031738\n",
      "encode_time 0.0032088756561279297\n",
      "postprocessing_time 0.32370471954345703\n",
      "get_model_acts_time 0.046639442443847656\n",
      "encode_time 0.0020601749420166016\n",
      "postprocessing_time 0.3334481716156006\n",
      "get_model_acts_time 0.04807710647583008\n",
      "encode_time 0.0020880699157714844\n",
      "postprocessing_time 0.3358597755432129\n",
      "get_model_acts_time 0.045471906661987305\n",
      "encode_time 0.0022530555725097656\n",
      "postprocessing_time 0.3222811222076416\n",
      "get_model_acts_time 0.04291987419128418\n",
      "encode_time 0.0021011829376220703\n",
      "postprocessing_time 0.32944488525390625\n",
      "get_model_acts_time 0.04845929145812988\n",
      "encode_time 0.002105712890625\n",
      "postprocessing_time 0.33642053604125977\n",
      "get_model_acts_time 0.05005478858947754\n",
      "encode_time 0.0025207996368408203\n",
      "postprocessing_time 0.32832860946655273\n",
      "get_model_acts_time 0.05294466018676758\n",
      "encode_time 0.0020682811737060547\n",
      "postprocessing_time 0.33006954193115234\n",
      "get_model_acts_time 0.05032992362976074\n",
      "encode_time 0.002062559127807617\n",
      "postprocessing_time 0.3142051696777344\n",
      "get_model_acts_time 0.05004143714904785\n",
      "encode_time 0.0020503997802734375\n",
      "postprocessing_time 0.35063791275024414\n",
      "get_model_acts_time 0.05565619468688965\n",
      "encode_time 0.002104043960571289\n",
      "postprocessing_time 0.32431864738464355\n",
      "get_model_acts_time 0.05301332473754883\n",
      "encode_time 0.0022275447845458984\n",
      "postprocessing_time 0.3239312171936035\n",
      "get_model_acts_time 0.04895496368408203\n",
      "encode_time 0.002539396286010742\n",
      "postprocessing_time 0.32357287406921387\n",
      "get_model_acts_time 0.049439191818237305\n",
      "encode_time 0.002079010009765625\n",
      "postprocessing_time 0.3251826763153076\n",
      "get_model_acts_time 0.049764156341552734\n",
      "encode_time 0.0021212100982666016\n",
      "postprocessing_time 0.32543420791625977\n",
      "get_model_acts_time 0.047518253326416016\n",
      "encode_time 0.0020668506622314453\n",
      "postprocessing_time 0.32649898529052734\n",
      "get_model_acts_time 0.05294632911682129\n",
      "encode_time 0.002075672149658203\n",
      "postprocessing_time 0.3215160369873047\n",
      "get_model_acts_time 0.049685001373291016\n",
      "encode_time 0.0020666122436523438\n",
      "postprocessing_time 0.32227563858032227\n",
      "get_model_acts_time 0.04874420166015625\n",
      "encode_time 0.0026879310607910156\n",
      "postprocessing_time 0.3199939727783203\n",
      "get_model_acts_time 0.04738616943359375\n",
      "encode_time 0.002103567123413086\n",
      "postprocessing_time 0.331282377243042\n",
      "get_model_acts_time 0.0484471321105957\n",
      "encode_time 0.002065420150756836\n",
      "postprocessing_time 0.32431650161743164\n",
      "get_model_acts_time 0.050292015075683594\n",
      "encode_time 0.0020599365234375\n",
      "postprocessing_time 0.33718323707580566\n",
      "get_model_acts_time 0.04581117630004883\n",
      "encode_time 0.0020723342895507812\n",
      "postprocessing_time 0.33568859100341797\n",
      "get_model_acts_time 0.05442214012145996\n",
      "encode_time 0.0021719932556152344\n",
      "postprocessing_time 0.3255612850189209\n",
      "get_model_acts_time 0.04525351524353027\n",
      "encode_time 0.0025572776794433594\n",
      "postprocessing_time 0.3285083770751953\n",
      "get_model_acts_time 0.04587078094482422\n",
      "encode_time 0.0020585060119628906\n",
      "postprocessing_time 0.32592344284057617\n",
      "get_model_acts_time 0.04283022880554199\n",
      "encode_time 0.002086162567138672\n",
      "postprocessing_time 0.3414733409881592\n",
      "get_model_acts_time 0.049878835678100586\n",
      "encode_time 0.002067089080810547\n",
      "postprocessing_time 0.3270735740661621\n",
      "get_model_acts_time 0.04823136329650879\n",
      "encode_time 0.002095460891723633\n",
      "postprocessing_time 0.31905412673950195\n",
      "get_model_acts_time 0.04784750938415527\n",
      "encode_time 0.0020546913146972656\n",
      "postprocessing_time 0.3271753787994385\n",
      "get_model_acts_time 0.04554438591003418\n",
      "encode_time 0.002454519271850586\n",
      "postprocessing_time 0.3346381187438965\n",
      "get_model_acts_time 0.046488046646118164\n",
      "encode_time 0.002032041549682617\n",
      "postprocessing_time 0.34088134765625\n",
      "get_model_acts_time 0.06391096115112305\n",
      "encode_time 0.002109050750732422\n",
      "postprocessing_time 0.33461737632751465\n",
      "get_model_acts_time 0.048110246658325195\n",
      "encode_time 0.0020618438720703125\n",
      "postprocessing_time 0.3560948371887207\n",
      "get_model_acts_time 0.04300689697265625\n",
      "encode_time 0.0020542144775390625\n",
      "postprocessing_time 0.3402726650238037\n",
      "get_model_acts_time 0.042641639709472656\n",
      "encode_time 0.002079010009765625\n",
      "postprocessing_time 0.333066463470459\n",
      "get_model_acts_time 0.045953989028930664\n",
      "encode_time 0.0025446414947509766\n",
      "postprocessing_time 0.3303239345550537\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m      3\u001b[0m test_feature_idx_llama \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m([\u001b[38;5;241m2705\u001b[39m, \u001b[38;5;241m9766\u001b[39m, \u001b[38;5;241m18472\u001b[39m, \u001b[38;5;241m22648\u001b[39m, \u001b[38;5;241m24905\u001b[39m, \u001b[38;5;241m25939\u001b[39m, \u001b[38;5;241m27169\u001b[39m, \u001b[38;5;241m27353\u001b[39m, \u001b[38;5;241m27368\u001b[39m, \u001b[38;5;241m32379\u001b[39m])\n\u001b[1;32m      5\u001b[0m feature_vis_config_llama \u001b[38;5;241m=\u001b[39m SaeVisConfig(\n\u001b[1;32m      6\u001b[0m     hook_point\u001b[38;5;241m=\u001b[39msae\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mhook_name,\n\u001b[1;32m      7\u001b[0m     features\u001b[38;5;241m=\u001b[39mtest_feature_idx_llama,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbfloat16\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mSaeVisRunner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_vis_config_llama\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/datadrive5/huypn16/anaconda3/envs/ana/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/datadrive5/huypn16-backup/ReinforceLLM/SAEDashboard/sae_dashboard/sae_vis_runner.py:130\u001b[0m, in \u001b[0;36mSaeVisRunner.run\u001b[0;34m(self, encoder, model, tokens)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m    121\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    122\u001b[0m (\n\u001b[1;32m    123\u001b[0m     all_feat_acts,\n\u001b[1;32m    124\u001b[0m     _,  \u001b[38;5;66;03m# all resid post. no longer used.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     feature_resid_dir,\n\u001b[1;32m    126\u001b[0m     feature_out_dir,\n\u001b[1;32m    127\u001b[0m     corrcoef_neurons,\n\u001b[1;32m    128\u001b[0m     corrcoef_encoder,\n\u001b[1;32m    129\u001b[0m     batch_dfa_results,\n\u001b[0;32m--> 130\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_data_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_feat_acts.shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, all_feat_acts\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_resid_dir.shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, feature_resid_dir\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/datadrive5/huypn16/anaconda3/envs/ana/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/datadrive5/huypn16-backup/ReinforceLLM/SAEDashboard/sae_dashboard/feature_data_generator.py:86\u001b[0m, in \u001b[0;36mFeatureDataGenerator.get_feature_data\u001b[0;34m(self, feature_indices, progress)\u001b[0m\n\u001b[1;32m     84\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     85\u001b[0m minibatch\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 86\u001b[0m model_activation_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model_acts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminibatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_model_acts_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0)\n\u001b[1;32m     89\u001b[0m primary_acts \u001b[38;5;241m=\u001b[39m model_activation_dict[\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mactivation_config\u001b[38;5;241m.\u001b[39mprimary_hook_point\n\u001b[1;32m     91\u001b[0m ]\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m     93\u001b[0m )  \u001b[38;5;66;03m# make sure acts are on the correct device\u001b[39;00m\n",
      "File \u001b[0;32m/datadrive5/huypn16/anaconda3/envs/ana/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/datadrive5/huypn16-backup/ReinforceLLM/SAEDashboard/sae_dashboard/feature_data_generator.py:171\u001b[0m, in \u001b[0;36mFeatureDataGenerator.get_model_acts\u001b[0;34m(self, minibatch_index, minibatch_tokens, use_cache)\u001b[0m\n\u001b[1;32m    169\u001b[0m cache_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mcache_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_activations_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mminibatch_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mand\u001b[39;00m cache_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m--> 171\u001b[0m     activation_dict \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor_dict_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    173\u001b[0m     activation_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m    174\u001b[0m         minibatch_tokens\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m), return_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     )\n",
      "File \u001b[0;32m/datadrive5/huypn16-backup/ReinforceLLM/SAEDashboard/sae_dashboard/feature_data_generator.py:224\u001b[0m, in \u001b[0;36mload_tensor_dict_torch\u001b[0;34m(filename, device)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_tensor_dict_torch\u001b[39m(filename: Path, device: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/datadrive5/huypn16/anaconda3/envs/ana/lib/python3.11/site-packages/torch/serialization.py:1097\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1095\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1096\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1097\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1105\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/datadrive5/huypn16/anaconda3/envs/ana/lib/python3.11/site-packages/torch/serialization.py:1525\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# Needed for tensors where storage device and rebuild tensor device are\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# not connected (wrapper subclasses and tensors rebuilt using numpy)\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_thread_local_state\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 1525\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_thread_local_state\u001b[38;5;241m.\u001b[39mmap_location\n\u001b[1;32m   1528\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m/datadrive5/huypn16/anaconda3/envs/ana/lib/python3.11/site-packages/torch/serialization.py:1492\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1491\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1492\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m/datadrive5/huypn16/anaconda3/envs/ana/lib/python3.11/site-packages/torch/serialization.py:1457\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     storage \u001b[38;5;241m=\u001b[39m overall_storage[storage_offset:storage_offset \u001b[38;5;241m+\u001b[39m numel]\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;66;03m# swap here if byteswapping is needed\u001b[39;00m\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m byteorderdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "test_feature_idx_llama = list([2705, 9766, 18472, 22648, 24905, 25939, 27169, 27353, 27368, 32379])\n",
    "\n",
    "feature_vis_config_llama = SaeVisConfig(\n",
    "    hook_point=sae.cfg.hook_name,\n",
    "    features=test_feature_idx_llama,\n",
    "    minibatch_size_features=10,\n",
    "    minibatch_size_tokens=16,  # this is number of prompts at a time.\n",
    "    verbose=True,\n",
    "    device=\"cuda\",\n",
    "    cache_dir=Path(\n",
    "        \"llama.layers.8_bs=128_nrows=16384\"\n",
    "    ),  # this will enable us to skip running the model for subsequent features.\n",
    "    dtype=\"bfloat16\",\n",
    ")\n",
    "\n",
    "data = SaeVisRunner(feature_vis_config_llama).run(\n",
    "    encoder=sae,  # type: ignore\n",
    "    model=model,\n",
    "    tokens=token_dataset[:4096],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc316e3c99844f2b23e4aa986aac7a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving feature-centric vis:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sae_dashboard.data_writing_fns import save_feature_centric_vis\n",
    "\n",
    "filename = f\"llama.layers.8.thresholdfire_toks=16384x1024.html\"\n",
    "save_feature_centric_vis(sae_vis_data=data, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ana",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
