{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Download SAE with SAE Lens.\n",
    "2. Create a dataset consistent with that SAE. \n",
    "3. Fold the SAE decoder norm weights so that feature activations are \"correct\".\n",
    "4. Estimate the activation normalization constant if needed, and fold it into the SAE weights.\n",
    "5. Run the SAE generator for the features you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import json\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import torch\n",
    "from sae_lens import SAE\n",
    "from transformers import AutoTokenizer\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_dashboard.sae_vis_data import SaeVisConfig\n",
    "from sae_dashboard.sae_vis_runner import SaeVisRunner\n",
    "from sae_dashboard.data_writing_fns import save_prompt_centric_vis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Oct 18 17:49:52 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.42.06              Driver Version: 555.42.06      CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off |   00000001:00:00.0 Off |                    0 |\n",
      "| N/A   38C    P0             74W /  300W |   69475MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          Off |   00000002:00:00.0 Off |                    0 |\n",
      "| N/A   37C    P0             52W /  300W |     121MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1962      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    0   N/A  N/A    471127      C   ...ypn16/anaconda3/envs/ana/bin/python      69452MiB |\n",
      "|    1   N/A  N/A      1962      G   /usr/lib/xorg/Xorg                             87MiB |\n",
      "|    1   N/A  N/A   2328009      G   /usr/bin/gnome-shell                            9MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b4048f7266842caaeb8c7310b5ee643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "layers.14/cfg.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Loading from eleuther-----\n",
      "/datadrive5/.cache/hub/models--huypn16--sae-qwen-2.5-1.5B-OMS-16x/snapshots/af2cd1bd1a7cfc848469a0a7c70d2baa23e0eb56/layers.14/cfg.json\n",
      "{'model_name': 'Qwen/Qwen2.5-1.5B-Instruct', 'architecture': 'topk', 'hook_name': 'blocks.14.hook_resid_post', 'hook_layer': 14, 'layer': 14, 'k': 64, 'activation_fn_str': 'relu', 'd_sae': 24576, 'd_in': 1536, 'multi_topk': False, 'device': 'cuda', 'apply_b_dec_to_input': False, 'finetuning_scaling_factor': False, 'context_size': 1024, 'hook_head_index': None, 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'float32', 'sae_lens_training_version': 'eleuther', 'neuronpedia_id': None, 'activation_fn_kwargs': {}, 'model_from_pretrained_kwargs': {}}\n",
      "{'model_name': 'Qwen/Qwen2.5-1.5B-Instruct', 'architecture': 'topk', 'hook_name': 'blocks.14.hook_resid_post', 'hook_layer': 14, 'layer': 14, 'k': 64, 'activation_fn_str': 'relu', 'd_sae': 24576, 'd_in': 1536, 'multi_topk': False, 'device': 'cuda', 'apply_b_dec_to_input': False, 'finetuning_scaling_factor': False, 'context_size': 1024, 'hook_head_index': None, 'prepend_bos': True, 'normalize_activations': 'none', 'dtype': 'float32', 'sae_lens_training_version': 'eleuther', 'neuronpedia_id': None, 'activation_fn_kwargs': {}, 'model_from_pretrained_kwargs': {}, 'dataset_path': '', 'dataset_trust_remote_code': False}\n",
      "Loading model config for Qwen/Qwen2.5-1.5B-Instruct\n",
      "Loaded model config for {'d_model': 1536, 'd_head': 128, 'n_heads': 12, 'n_key_value_heads': 2, 'd_mlp': 8960, 'n_layers': 28, 'n_ctx': 2048, 'eps': 1e-06, 'd_vocab': 151936, 'act_fn': 'silu', 'use_attn_scale': True, 'initializer_range': 0.02, 'normalization_type': 'RMS', 'positional_embedding_type': 'rotary', 'rotary_base': 1000000.0, 'rotary_adjacent_pairs': False, 'rotary_dim': 128, 'tokenizer_prepends_bos': True, 'final_rms': True, 'gated_mlp': True, 'original_architecture': 'Qwen2ForCausalLM', 'tokenizer_name': 'Qwen/Qwen2.5-1.5B-Instruct'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen2.5-1.5B-Instruct into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "sae, cfg_dict, sparsity = SAE.from_eleuther(\n",
    "    release=\"huypn16/sae-qwen-2.5-1.5B-OMS-16x\",  # see other options in sae_lens/pretrained_saes.yaml\n",
    "    sae_id=\"layers.14\",  # won't always be a hook point\n",
    "    device=device,\n",
    ") # type: ignore\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\", device=device, dtype=\"bfloat16\")\n",
    "# fold w_dec norm so feature activations are accurate\n",
    "sae.fold_W_dec_norm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path, databilder:  lighteval/MATH\n",
      "Token columns: ['problem', 'solution']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datadrive5/huypn16/anaconda3/envs/ana/lib/python3.11/site-packages/sae_lens/training/activations_store.py:265: UserWarning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://jbloomaus.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sae_lens import ActivationsStore\n",
    "activations_store = ActivationsStore.from_sae(\n",
    "    model=model,\n",
    "    sae=sae,\n",
    "    streaming=True,\n",
    "    dataset=\"lighteval/MATH\",\n",
    "    token_columns=[\"problem\", \"solution\"],\n",
    "    store_batch_size_prompts=16,\n",
    "    n_batches_in_buffer=16,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_end|> [asy]\n",
      "\n",
      "pair X,Y,Z;\n",
      "\n",
      "Z = (0,0);\n",
      "\n",
      "Y = (sqrt(51),0);\n",
      "\n",
      "X = (0,7);\n",
      "\n",
      "draw(X--Y--Z--X);\n",
      "\n",
      "draw(rightanglemark(Y,Z,X,15));\n",
      "\n",
      "label(\"$X$\",X,NE);\n",
      "\n",
      "label(\"$Y$\",Y,SE);\n",
      "\n",
      "label(\"$Z$\",Z,SW);\n",
      "\n",
      "label(\"$10$\",(X+Y)/2,NE);\n",
      "\n",
      "label(\"$\\sqrt{51}$\",(Z+Y)/2,S);\n",
      "\n",
      "[/asy]\n",
      "\n",
      "Because this is a right triangle, $\\tan X = \\frac{YZ}{XZ}$.\n",
      "\n",
      "Using the Pythagorean Theorem, we find $XZ = \\sqrt{XY^2 - YZ^2} = \\sqrt{100-51} = 7$.\n",
      "\n",
      "So $\\tan X = \\boxed{\\frac{\\sqrt{51}}{7}}$.<|im_end|>problem: The points $B(1, 1)$, $I(2, 4)$ and $G(5, 1)$ are plotted in the standard rectangular coordinate system to form triangle $BIG$. Triangle $BIG$ is translated five units to the left and two units upward to triangle $B'I'G'$, in such a way that $B'$ is the image of $B$, $I'$ is the image of $I$, and $G'$ is the image of $G$. What is the midpoint of segment $B'G'$? Express your answer as an ordered pair.\n",
      "solution: Since triangle $B^\\prime I^\\prime G^\\prime$ is translated from triangle $BIG,$ the midpoint of $B^\\prime G ^\\prime $ is the midpoint of $BG$ translated five units left and two units up. The midpoint of $BG$ is at $\\left( \\frac{1+5}{2}, \\frac{1+1}{2} \\right) = (3, 1).$ Thus, the midpoint of $B ^\\prime G ^\\prime$ is at $(3-5,1+2)=\\boxed{(-2,3)}.$<|im_end|>problem: How many cubic feet are in the volume of a round swimming pool which is 16 feet in diameter and 4 feet deep throughout? Express your answer in terms of $\\pi$.\n",
      "solution: The radius of this pool is $16/2=8$ feet; the volume of this pool is thus $\\pi(8^2)(4)=\\boxed{256\\pi}$ cubic feet.<|im_end|>problem: Side $AB$ of regular hexagon $ABCDEF$ is extended past $B$ to point $X$ such that $AX = 3AB$. Given that each side of the hexagon is $2$ units long, what is the length of segment $FX$? Express your answer in simplest radical form.\n",
      "solution: Let $P$ be the foot of the perpendicular from $F$ to the line containing $AB$. [asy]size(150);\n",
      "defaultpen(linewidth(0.7) + fontsize(10)); real lsf = 0.6;\n",
      "pair C = (2,0), B = 2*dir(60), A = 2*dir(120), F = -C, E = -B, D = -A, P = foot(F,A,B), Y = B+(4,0);\n",
      "draw(A--B--C--D--E--F--cycle); draw(F--P--Y--cycle); draw(rightanglemark(F,P,A,5));\n",
      "label(\"$A$\",A,lsf*A); label(\"$B$\",B,lsf*B); label(\"$C$\",C,lsf*C); label(\"$D$\",D,lsf*D); label(\"$E$\",E,lsf*E); label(\"$F$\",F,lsf*F); label(\"$P$\",P,N); label(\"$X$\",Y,N);\n",
      "[/asy] Since $\\angle FAB = 120^{\\circ},$ then $\\angle PAF = 180^\\circ - 120^\\circ = 60^{\\circ}$, and it follows that $\\triangle PAF$ is a $30-60-90$ triangle. As $AF = 2$, it follows that $AP = 1$ and $PF = \\sqrt{3}$. Also, $AB = 2$ and so $AX = 3AB = 6$. Thus, $PX = AP + AX = 7$. In right triangle $FPX$, by the Pythagorean Theorem, it follows that $$FX^2 = PF^2 + PX^2 = (\\sqrt{3})^2 + (7)^2 = 52,$$and $FX = \\sqrt{52} = \\boxed{2\\sqrt{13}}$.<|im_end|>problem\n",
      "torch.Size([32768, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_895994/1245206191.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  token_dataset = torch.load(\"token_dataset_lighteval.pt\")\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def get_tokens(\n",
    "    activations_store: ActivationsStore,\n",
    "    n_prompts: int,\n",
    "):\n",
    "    all_tokens_list = []\n",
    "    pbar = tqdm(range(n_prompts))\n",
    "    for _ in pbar:\n",
    "        batch_tokens = activations_store.get_batch_tokens()\n",
    "        batch_tokens = batch_tokens[torch.randperm(batch_tokens.shape[0])][\n",
    "            : batch_tokens.shape[0]\n",
    "        ]\n",
    "        all_tokens_list.append(batch_tokens)\n",
    "\n",
    "    all_tokens = torch.cat(all_tokens_list, dim=0)\n",
    "    all_tokens = all_tokens[torch.randperm(all_tokens.shape[0])]\n",
    "    return all_tokens\n",
    "\n",
    "# 1000 prompts is plenty for a demo.\n",
    "if os.path.exists(\"token_dataset_lighteval.pt\"):\n",
    "    token_dataset = torch.load(\"token_dataset_lighteval.pt\")\n",
    "else:\n",
    "    token_dataset = get_tokens(activations_store, n_prompts=2048)\n",
    "    torch.save(token_dataset, \"token_dataset_lighteval.pt\")\n",
    "\n",
    "print(tokenizer.decode(token_dataset[0]))\n",
    "print(token_dataset.shape) # [store_batch_size_prompts * n_prompts, 1024]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 Evaluate the SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard replacement hook:  blocks.14.hook_resid_post\n",
      "standard replacement hook:  blocks.14.hook_resid_post\n",
      "standard replacement hook:  blocks.14.hook_resid_post\n",
      "standard replacement hook:  blocks.14.hook_resid_post\n",
      "standard replacement hook:  blocks.14.hook_resid_post\n",
      "standard replacement hook:  blocks.14.hook_resid_post\n",
      "standard replacement hook:  blocks.14.hook_resid_post\n",
      "standard replacement hook:  blocks.14.hook_resid_post\n",
      "standard replacement hook:  blocks.14.hook_resid_post\n",
      "standard replacement hook:  blocks.14.hook_resid_post\n",
      "{\n",
      "    \"metrics/kl_div_with_sae\": 0.310546875,\n",
      "    \"metrics/kl_div_with_ablation\": 16.5,\n",
      "    \"metrics/ce_loss_with_sae\": 1.109375,\n",
      "    \"metrics/ce_loss_without_sae\": 0.79296875,\n",
      "    \"metrics/ce_loss_with_ablation\": 17.25,\n",
      "    \"metrics/kl_div_score\": 0.9811789772727273,\n",
      "    \"metrics/ce_loss_score\": 0.9807737953952054,\n",
      "    \"metrics/l2_norm_in\": 62.75,\n",
      "    \"metrics/l2_norm_out\": 80.74810028076172,\n",
      "    \"metrics/l2_ratio\": 1.3440078496932983,\n",
      "    \"metrics/l0\": 64.0,\n",
      "    \"metrics/l1\": 332.2187194824219,\n",
      "    \"metrics/explained_variance\": 0.3974838852882385,\n",
      "    \"metrics/mse\": 0.1363437920808792,\n",
      "    \"metrics/total_tokens_evaluated\": 81920\n",
      "}\n",
      "0.9807737953952054\n",
      "0.79296875\n",
      "1.109375\n"
     ]
    }
   ],
   "source": [
    "from sae_lens import run_evals\n",
    "from sae_lens.evals import get_eval_everything_config\n",
    "\n",
    "eval_metrics = run_evals(\n",
    "    sae=sae,\n",
    "    activation_store=activations_store,\n",
    "    model=model,\n",
    "    eval_config=get_eval_everything_config(\n",
    "        batch_size_prompts=8,\n",
    "        n_eval_reconstruction_batches=10,\n",
    "        n_eval_sparsity_variance_batches=10,\n",
    "    )\n",
    ")\n",
    "print(json.dumps(eval_metrics, indent=4))\n",
    "# CE Loss score should be high for residual stream SAEs\n",
    "print(eval_metrics[\"metrics/ce_loss_score\"])\n",
    "# ce loss without SAE should be fairly low < 3.5 suggesting the Model is being run correctly\n",
    "print(eval_metrics[\"metrics/ce_loss_without_sae\"])\n",
    "# ce loss with SAE shouldn't be massively higher\n",
    "print(eval_metrics[\"metrics/ce_loss_with_sae\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Generate Feature Dashboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "# random features\n",
    "test_feature_idx_qwen = random.sample(range(sae.cfg.d_sae), 4)\n",
    "# test_feature_idx_qwen = [21719]\n",
    "# test_feature_idx_qwen = random.sample(range(1024), 2)\n",
    "# test_feature_idx_qwen = [1, 2, 3]\n",
    "# test_feature_idx_qwen =  [101, 345, 4087, 4297, 4410, 4411, 4444, 4782, 4783, 4877, 4954, 6460, 6551, 6878, 7384, 7410, 8303, 9321, 9775, 10327, 10738, 11302, 11594, 13107, 14068, 14344, 15023, 15311, 16451, 17808, 17975, 18038, 18312, 18758, 18923, 20166, 21021, 21719, 21792, 21850, 22720, 23602, 24219]\n",
    "# test_feature_idx_qwen = range(sae.cfg.d_sae)\n",
    "\n",
    "feature_vis_config_llama = SaeVisConfig(\n",
    "    hook_point=sae.cfg.hook_name,\n",
    "    features=test_feature_idx_qwen,\n",
    "    minibatch_size_features=4,\n",
    "    minibatch_size_tokens=256,  # this is number of prompts at a time.\n",
    "    verbose=True,\n",
    "    device=\"cuda\",\n",
    "    cache_dir=Path(\n",
    "        \"math.qwen25.layers.14_bs=256_nrows=8192\" # dataset.model.layer.bs.nrows\n",
    "    ),  # this will enable us to skip running the model for subsequent features.\n",
    "    dtype=\"bfloat16\",\n",
    ")\n",
    "\n",
    "data = SaeVisRunner(feature_vis_config_llama).run(\n",
    "    encoder=sae,  # type: ignore\n",
    "    model=model,\n",
    "    tokens=token_dataset[:8192],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename_4 = f\"prompt.qwen_math.layers.14.math_toks=16384x1024_complexsent_step4.html\"\n",
    "# filename_5 = f\"prompt.qwen_math.layers.14.math_toks=16384x1024_complexsent_step5.html\"\n",
    "\n",
    "# prompt = \"Solving the following mathematical problem. Problem: Calculate the following expression: (12 + 1000 * 2 - 1 ) * 412 - 2. Step 1: First, we minus 2 and 1\"\n",
    "\n",
    "# prompt = \"\"\"Problem:  Let $(a_1,b_1),$ $(a_2,b_2),$ $\\dots,$ $(a_n,b_n)$ be all the ordered pairs $(a,b)$ of complex numbers with $a^2+b^2\\\\neq 0,$\n",
    "# \\[a+\\\\frac{10b}{a^2+b^2}=5, \\quad \\\\text{and} \\quad b+\\\\frac{10a}{a^2+b^2}=4.\\]Find $a_1 + b_1 + a_2 + b_2 + \\dots + a_n + b_n.$ \n",
    "\n",
    "# #### Step 1: If $a = 0,$ then $\\\\frac{10}{b} = 5,$ so $b = 2,$ which does not satisfy the second equation.\n",
    "\n",
    "# #### Step 2: If $b = 0,$ then $\\\\frac{10}{a} = 4,$ so $a = \\\\frac{5}{2},$ which does not satisfy the first equation.\n",
    "\n",
    "# #### Step 3: So, we can assume that both $a$ and $b$ are nonzero.\n",
    "\n",
    "# #### Step 4: Then $\\\\frac{5 - a}{b} = \\\\frac{4 - b}{a} = \\\\frac{10}{a^2 + b^2}.$\n",
    "\n",
    "# #### Step 5: \\[\\\\frac{5b - ab}{b^2} = \\\\frac{4a - ab}{a^2} = \\\\frac{10}{a^2 + b^2},\\]so\n",
    "# \\[\\\\frac{4a + 5b - 2ab}{a^2 + b^2} = \\\\frac{10}{a^2 + b^2},\\]so $4a + 5b - 2ab = 10.$\n",
    "\n",
    "# #### Step 6: Then $2ab - 4a - 5b + 10 = 0,$ which factors as $(2a - 5)(b - 2) = 0.$  Hence, $a = \\\\frac{5}{2}$ or $b = 2.$\"\"\"\n",
    "\n",
    "prompt = \"\"\"Problem:  Find all the ordered pairs $(a,b)$ of complex numbers with $a^2+b^2\\\\neq 0,\n",
    "$\\[a+\\\\frac{10b}{a^2+b^2}=5, \\\\text{and} b+\\\\frac{10a}{a^2+b^2}=4.\\]$\"\"\"\n",
    "\n",
    "suffix = \"#### Step 1: If $a = 0,$ then $\\\\frac{10}{b} = 5,$ so $b = 2$.\"\n",
    "\n",
    "# ------------------------------------- Step 1 -------------------------------------\n",
    "# prompt = \"\"\"Problem:  Let $(a_1,b_1),$ $(a_2,b_2),$ $\\dots,$ $(a_n,b_n)$ be all the ordered pairs $(a,b)$ of complex numbers with $a^2+b^2\\\\neq 0,$\n",
    "# \\[a+\\\\frac{10b}{a^2+b^2}=5, \\quad \\\\text{and} \\quad b+\\\\frac{10a}{a^2+b^2}=4.\\]Find $a_1 + b_1 + a_2 + b_2 + \\dots + a_n + b_n.$ \"\"\"\n",
    "\n",
    "# suffix = \"#### Step 1: If $a = 0,$ then $\\\\frac{10}{b} = 5,$ so $b = 2,$ which does not satisfy the second equation.\"\n",
    "\n",
    "# ------------------------------------- Step 2 -------------------------------------\n",
    "# prompt = \"\"\"Problem:  Let $(a_1,b_1),$ $(a_2,b_2),$ $\\dots,$ $(a_n,b_n)$ be all the ordered pairs $(a,b)$ of complex numbers with $a^2+b^2\\\\neq 0,$\n",
    "# \\[a+\\\\frac{10b}{a^2+b^2}=5, \\quad \\\\text{and} \\quad b+\\\\frac{10a}{a^2+b^2}=4.\\]Find $a_1 + b_1 + a_2 + b_2 + \\dots + a_n + b_n.$ \n",
    "\n",
    "# #### Step 1: If $a = 0,$ then $\\\\frac{10}{b} = 5,$ so $b = 2,$ which does not satisfy the second equation.\"\"\"\n",
    "\n",
    "# suffix = \"#### Step 2: If $b = 0,$ then $\\\\frac{10}{a} = 4,$ so $a = \\\\frac{5}{2},$ which does not satisfy the first equation.\"\n",
    "\n",
    "\n",
    "# ------------------------------------- Step 3 -------------------------------------\n",
    "# prompt = \"\"\"Problem:  Let $(a_1,b_1),$ $(a_2,b_2),$ $\\dots,$ $(a_n,b_n)$ be all the ordered pairs $(a,b)$ of complex numbers with $a^2+b^2\\\\neq 0,$\n",
    "# \\[a+\\\\frac{10b}{a^2+b^2}=5, \\quad \\\\text{and} \\quad b+\\\\frac{10a}{a^2+b^2}=4.\\]Find $a_1 + b_1 + a_2 + b_2 + \\dots + a_n + b_n.$ \n",
    "\n",
    "# #### Step 1: If $a = 0,$ then $\\\\frac{10}{b} = 5,$ so $b = 2,$ which does not satisfy the second equation.\n",
    "\n",
    "# #### Step 2: If $b = 0,$ then $\\\\frac{10}{a} = 4,$ so $a = \\\\frac{5}{2},$ which does not satisfy the first equation.\"\"\"\n",
    "\n",
    "# suffix = \"#### Step 3: So, we can assume that both $a$ and $b$ are nonzero.\"\n",
    "\n",
    "# ------------------------------------- Step 4 -------------------------------------\n",
    "# prompt_4 = \"\"\"Problem:  Let $(a_1,b_1),$ $(a_2,b_2),$ $\\dots,$ $(a_n,b_n)$ be all the ordered pairs $(a,b)$ of complex numbers with $a^2+b^2\\\\neq 0,$\n",
    "# \\[a+\\\\frac{10b}{a^2+b^2}=5, \\quad \\\\text{and} \\quad b+\\\\frac{10a}{a^2+b^2}=4.\\]Find $a_1 + b_1 + a_2 + b_2 + \\dots + a_n + b_n.$ \n",
    "\n",
    "# #### Step 1: If $a = 0,$ then $\\\\frac{10}{b} = 5,$ so $b = 2,$ which does not satisfy the second equation.\n",
    "\n",
    "# #### Step 2: If $b = 0,$ then $\\\\frac{10}{a} = 4,$ so $a = \\\\frac{5}{2},$ which does not satisfy the first equation.\n",
    "\n",
    "# #### Step 3: So, we can assume that both $a$ and $b$ are nonzero.\"\"\"\n",
    "\n",
    "# suffix_4 =  \"#### Step 4: Then $\\\\frac{5 - a}{b} = \\\\frac{4 - b}{a} = \\\\frac{10}{a^2 + b^2}.$\"\n",
    "# ------------------------------------- Step 5 -------------------------------------\n",
    "# prompt_5 = \"\"\"Problem:  Let $(a_1,b_1),$ $(a_2,b_2),$ $\\dots,$ $(a_n,b_n)$ be all the ordered pairs $(a,b)$ of complex numbers with $a^2+b^2\\\\neq 0,$\n",
    "# \\[a+\\\\frac{10b}{a^2+b^2}=5, \\quad \\\\text{and} \\quad b+\\\\frac{10a}{a^2+b^2}=4.\\]Find $a_1 + b_1 + a_2 + b_2 + \\dots + a_n + b_n.$ \n",
    "\n",
    "# #### Step 1: If $a = 0,$ then $\\\\frac{10}{b} = 5,$ so $b = 2,$ which does not satisfy the second equation.\n",
    "\n",
    "# #### Step 2: If $b = 0,$ then $\\\\frac{10}{a} = 4,$ so $a = \\\\frac{5}{2},$ which does not satisfy the first equation.\n",
    "\n",
    "# #### Step 3: So, we can assume that both $a$ and $b$ are nonzero.\n",
    "\n",
    "# #### Step 4: Then $\\\\frac{5 - a}{b} = \\\\frac{4 - b}{a} = \\\\frac{10}{a^2 + b^2}.$\"\"\"\n",
    "\n",
    "# suffix_5 = \"\"\"#### Step 5: \\[\\\\frac{5b - ab}{b^2} = \\\\frac{4a - ab}{a^2} = \\\\frac{10}{a^2 + b^2},\\]so\n",
    "# # \\[\\\\frac{4a + 5b - 2ab}{a^2 + b^2} = \\\\frac{10}{a^2 + b^2},\\]so $4a + 5b - 2ab = 10.$\"\"\"\n",
    "fn = f\"prompt.qwen_math.layers.14.math_toks=8192x1024_complexsent_step0.html\"\n",
    "save_prompt_centric_vis(prompt=prompt+suffix, sae_vis_data=data, filename=fn, num_top_features=20)\n",
    "# save_prompt_centric_vis(prompt=prompt_4+suffix_4, sae_vis_data=data, filename=filename_4, num_top_features=20)\n",
    "# save_prompt_centric_vis(prompt=prompt_5+suffix_5, sae_vis_data=data, filename=filename_5, num_top_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_dashboard.data_writing_fns import save_feature_centric_vis\n",
    "\n",
    "filename = f\"feature.qwen25.layers.14.complexsent.4096f.toks=16384x1024.html\"\n",
    "save_feature_centric_vis(sae_vis_data=data, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for displaying vis in Colab / notebook\n",
    "import webbrowser\n",
    "import http.server\n",
    "import socketserver\n",
    "import threading\n",
    "PORT = 8000\n",
    "\n",
    "def display_vis_inline(filename: str, height: int = 850):\n",
    "    '''\n",
    "    Displays the HTML files in Colab. Uses global `PORT` variable defined in prev cell, so that each\n",
    "    vis has a unique port without having to define a port within the function.\n",
    "    '''\n",
    "    webbrowser.open(filename);\n",
    "display_vis_inline(filename_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "from IPython import get_ipython # type: ignore\n",
    "ipython = get_ipython(); assert ipython is not None\n",
    "ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src=\"/datadrive5/huypn16-backup/ReinforceLLM/packages/SAEDashboard/prompt.qwen_math.layers.14.math_toks=16384x1024_complexsent_step1.html\", width=700, height=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ana",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
