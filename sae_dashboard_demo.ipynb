{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Download SAE with SAE Lens.\n",
    "2. Create a dataset consistent with that SAE. \n",
    "3. Fold the SAE decoder norm weights so that feature activations are \"correct\".\n",
    "4. Estimate the activation normalization constant if needed, and fold it into the SAE weights.\n",
    "5. Run the SAE generator for the features you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from sae_lens import SAE\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_dashboard.sae_vis_data import SaeVisConfig\n",
    "from sae_dashboard.sae_vis_runner import SaeVisRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Download / Initialize SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loading model config for meta-llama/Llama-3.2-1B-Instruct\n",
      "Loaded model config for {'d_model': 2048, 'd_head': 64, 'n_heads': 32, 'd_mlp': 8192, 'n_layers': 16, 'n_ctx': 2048, 'eps': 1e-05, 'd_vocab': 128256, 'act_fn': 'silu', 'n_key_value_heads': 8, 'normalization_type': 'RMS', 'positional_embedding_type': 'rotary', 'rotary_adjacent_pairs': False, 'rotary_dim': 64, 'final_rms': True, 'gated_mlp': True, 'original_architecture': 'LlamaForCausalLM', 'tokenizer_name': 'meta-llama/Llama-3.2-1B-Instruct'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.2-1B-Instruct into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datadrive5/huypn16/anaconda3/envs/ana/lib/python3.11/site-packages/sae_lens/sae.py:133: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n",
      "/datadrive5/huypn16/anaconda3/envs/ana/lib/python3.11/site-packages/sae_lens/sae.py:536: UserWarning: norm_scaling_factor not found for llama-3.2-1B-mlp-math and blocks.8.hook_mlp_out, but normalize_activations is 'expected_average_only_in'. Skipping normalization folding.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "model = HookedTransformer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", device=device, dtype=\"bfloat16\")\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=\"llama-3.2-1B-mlp-math\",  # see other options in sae_lens/pretrained_saes.yaml\n",
    "    sae_id=\"blocks.8.hook_mlp_out\",  # won't always be a hook point\n",
    "    device=device,\n",
    ")\n",
    "# fold w_dec norm so feature activations are accurate\n",
    "sae.fold_W_dec_norm()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get token dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datadrive5/huypn16/anaconda3/envs/ana/lib/python3.11/site-packages/sae_lens/training/activations_store.py:265: UserWarning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://jbloomaus.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token columns: ['problem']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estimating norm scaling factor: 100%|██████████| 30/30 [00:08<00:00,  3.67it/s]\n"
     ]
    }
   ],
   "source": [
    "from sae_lens import ActivationsStore\n",
    "activations_store = ActivationsStore.from_sae(\n",
    "    model=model,\n",
    "    sae=sae,\n",
    "    streaming=True,\n",
    "    dataset=\"lighteval/MATH\",\n",
    "    store_batch_size_prompts=16,\n",
    "    n_batches_in_buffer=8,\n",
    "    device=device,\n",
    ")\n",
    "# Some SAEs will require we estimate the activation norm and fold it into the weights. This is easy with SAE Lens.\n",
    "if sae.cfg.normalize_activations == \"expected_average_only_in\":\n",
    "    norm_scaling_factor = activations_store.estimate_norm_scaling_factor(\n",
    "        n_batches_for_norm_estimate=30\n",
    "    )\n",
    "    sae.fold_activation_norm_scaling_factor(norm_scaling_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [01:53<00:00,  9.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def get_tokens(\n",
    "    activations_store: ActivationsStore,\n",
    "    n_prompts: int,\n",
    "):\n",
    "    all_tokens_list = []\n",
    "    pbar = tqdm(range(n_prompts))\n",
    "    for _ in pbar:\n",
    "        batch_tokens = activations_store.get_batch_tokens()\n",
    "        batch_tokens = batch_tokens[torch.randperm(batch_tokens.shape[0])][\n",
    "            : batch_tokens.shape[0]\n",
    "        ]\n",
    "        all_tokens_list.append(batch_tokens)\n",
    "\n",
    "    all_tokens = torch.cat(all_tokens_list, dim=0)\n",
    "    all_tokens = all_tokens[torch.randperm(all_tokens.shape[0])]\n",
    "    return all_tokens\n",
    "\n",
    "# 1000 prompts is plenty for a demo.\n",
    "token_dataset = get_tokens(activations_store, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 Evaluate the SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"metrics/kl_div_with_sae\": 0.06201171875,\n",
      "    \"metrics/kl_div_with_ablation\": 0.333984375,\n",
      "    \"metrics/ce_loss_with_sae\": 1.9921875,\n",
      "    \"metrics/ce_loss_without_sae\": 1.9609375,\n",
      "    \"metrics/ce_loss_with_ablation\": 2.25,\n",
      "    \"metrics/kl_div_score\": 0.814327485380117,\n",
      "    \"metrics/ce_loss_score\": 0.8918918918918919,\n",
      "    \"metrics/l2_norm_in\": 3.28125,\n",
      "    \"metrics/l2_norm_out\": 2.46875,\n",
      "    \"metrics/l2_ratio\": 0.75,\n",
      "    \"metrics/l0\": 1366.159423828125,\n",
      "    \"metrics/l1\": 229.0,\n",
      "    \"metrics/explained_variance\": 0.6875,\n",
      "    \"metrics/mse\": 0.0007476806640625,\n",
      "    \"metrics/total_tokens_evaluated\": 40960\n",
      "}\n",
      "{'metrics/kl_div_with_sae': 0.06201171875, 'metrics/kl_div_with_ablation': 0.333984375, 'metrics/ce_loss_with_sae': 1.9921875, 'metrics/ce_loss_without_sae': 1.9609375, 'metrics/ce_loss_with_ablation': 2.25, 'metrics/kl_div_score': 0.814327485380117, 'metrics/ce_loss_score': 0.8918918918918919, 'metrics/l2_norm_in': 3.28125, 'metrics/l2_norm_out': 2.46875, 'metrics/l2_ratio': 0.75, 'metrics/l0': 1366.159423828125, 'metrics/l1': 229.0, 'metrics/explained_variance': 0.6875, 'metrics/mse': 0.0007476806640625, 'metrics/total_tokens_evaluated': 40960}\n",
      "0.8918918918918919\n",
      "1.9609375\n",
      "1.9921875\n"
     ]
    }
   ],
   "source": [
    "from sae_lens import run_evals\n",
    "from sae_lens.evals import get_eval_everything_config\n",
    "\n",
    "eval_metrics = run_evals(\n",
    "    sae=sae,\n",
    "    activation_store=activations_store,\n",
    "    model=model,\n",
    "    eval_config=get_eval_everything_config(\n",
    "        batch_size_prompts=8,\n",
    "        n_eval_reconstruction_batches=10,\n",
    "        n_eval_sparsity_variance_batches=3,\n",
    "    )\n",
    ")\n",
    "print(json.dumps(eval_metrics, indent=4))\n",
    "# CE Loss score should be high for residual stream SAEs\n",
    "print(eval_metrics[\"metrics/ce_loss_score\"])\n",
    "# ce loss without SAE should be fairly low < 3.5 suggesting the Model is being run correctly\n",
    "print(eval_metrics[\"metrics/ce_loss_without_sae\"])\n",
    "# ce loss with SAE shouldn't be massively higher\n",
    "print(eval_metrics[\"metrics/ce_loss_with_sae\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Generate Feature Dashboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111b6513beaf47ce99c2ee69536d6b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Forward passes to cache data for vis:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6827db157a485d9e3c7e2d1b52f574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting vis data from cached data:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━┳━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Task </span>┃<span style=\"font-weight: bold\"> Time </span>┃<span style=\"font-weight: bold\"> Pct % </span>┃\n",
       "┡━━━━━━╇━━━━━━╇━━━━━━━┩\n",
       "└──────┴──────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━┳━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTask\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mTime\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mPct %\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━╇━━━━━━╇━━━━━━━┩\n",
       "└──────┴──────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "test_feature_idx_llama = list([2705, 9766, 18472, 22648, 24905, 25939, 27169, 27353, 27368, 32379])\n",
    "\n",
    "feature_vis_config_llama = SaeVisConfig(\n",
    "    hook_point=sae.cfg.hook_name,\n",
    "    features=test_feature_idx_llama,\n",
    "    minibatch_size_features=10,\n",
    "    minibatch_size_tokens=256,  # this is number of prompts at a time.\n",
    "    verbose=True,\n",
    "    device=\"cuda\",\n",
    "    cache_dir=Path(\n",
    "        \"llama_original.layers.8_bs=256_nrows=16384\"\n",
    "    ),  # this will enable us to skip running the model for subsequent features.\n",
    "    dtype=\"bfloat16\",\n",
    ")\n",
    "\n",
    "data = SaeVisRunner(feature_vis_config_llama).run(\n",
    "    encoder=sae,  # type: ignore\n",
    "    model=model,\n",
    "    tokens=token_dataset[:16384], # 16384=1024 * 16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb2439323c4c44188300e6002e11c78a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving feature-centric vis:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sae_dashboard.data_writing_fns import save_feature_centric_vis\n",
    "\n",
    "filename = f\"demo_feature_dashboards.html\"\n",
    "save_feature_centric_vis(sae_vis_data=data, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ana",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
